{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Livrable** : Un ou des notebooks (ou des fichiers .py) contenant les fonctions permettant le prétraitement et la feature extraction des données textes et images ainsi que les résultats de l’étude de faisabilité (graphiques, mesure de similarité) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture du jeu de données flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the flipkart.csv file\n",
    "flipkart = pd.read_csv('pj1_flipkart.csv', encoding='utf-8', encoding_errors='replace', parse_dates=['crawl_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 15 columns):\n",
      " #   Column                   Non-Null Count  Dtype              \n",
      "---  ------                   --------------  -----              \n",
      " 0   uniq_id                  1050 non-null   object             \n",
      " 1   crawl_timestamp          1050 non-null   datetime64[ns, UTC]\n",
      " 2   product_url              1050 non-null   object             \n",
      " 3   product_name             1050 non-null   object             \n",
      " 4   product_category_tree    1050 non-null   object             \n",
      " 5   pid                      1050 non-null   object             \n",
      " 6   retail_price             1049 non-null   float64            \n",
      " 7   discounted_price         1049 non-null   float64            \n",
      " 8   image                    1050 non-null   object             \n",
      " 9   is_FK_Advantage_product  1050 non-null   bool               \n",
      " 10  description              1050 non-null   object             \n",
      " 11  product_rating           1050 non-null   object             \n",
      " 12  overall_rating           1050 non-null   object             \n",
      " 13  brand                    712 non-null    object             \n",
      " 14  product_specifications   1049 non-null   object             \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(2), object(11)\n",
      "memory usage: 116.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the flipkart DataFrame\n",
    "print(flipkart.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaires sur le fichier flipkart après visualisation sur le data wrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Champs                    | Description                                                                                  | Utile |\n",
    "|---------------------------|----------------------------------------------------------------------------------------------|-------|\n",
    "| uniq_id                   | Nom des fichiers images dans le répertoire image                                             | O     |\n",
    "| crawl_timestamp           | Timestamp du crawler qui a récupéré les images                                               | N     |\n",
    "| product_url               | Lien URL contenant le product_name & le PID                          | N     |\n",
    "| product_name              | Nom du produit extrait de l'URL                                                             | O     |\n",
    "| product_category_tree     | Fil d'ariane à extraire de la catégorie de produit categorical                               | O     |\n",
    "| pid                       | Product ou Page ID                                                                                   | N     |\n",
    "| retail_price              | Prix                                                                                         | O     |\n",
    "| discounted_price          | Prix réduit                                                                                  | O     |\n",
    "| image                     | La même chose que uniq_id mais avec le format à la fin                                       | O     |\n",
    "| is_FK_Advantage_product   | Série booléenne de ?                                                                         | N     |\n",
    "| description               | Description string du produit                                                                | O     |\n",
    "| product_rating            | Note moyenne, attention les valeurs nulles sont remplacées par 'No rating available'         | O     |\n",
    "| overall_rating            | Semble identique                                                                             | N     |\n",
    "| brand                     | La marque, contient beaucoup de valeurs manquantes, peut être récupéré du product name       | N     |\n",
    "| product_specifications    | Sous forme de K-V pair sur les key feature du produit                                        | O     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vérification sur les champs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En plus du fichier flipkart, nous avons un répertoire contenant les images des produits. <br>\n",
    "**Nous allons vérifier si les images dans le répertoire correspondent aux images dans le fichier flipkart** pour pouvoir ensuite rapprocher le dataset avec les images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers dans le répertoire Images:  1050\n",
      "La colonne uniq_id contient les mêmes noms que les fichiers dans le répertoire Images\n"
     ]
    }
   ],
   "source": [
    "# Count the number of files in the current directory\n",
    "print(\"Nombre de fichiers dans le répertoire Images: \", len(os.listdir(\"./Images\")))\n",
    "\n",
    "# Check if the number of lines in the flipkart.csv file is equal to the number of files in the Images directory\n",
    "assert (\n",
    "    len(os.listdir(\"./Images\")) == len(flipkart)\n",
    "), \"Le nombre de lignes ne corresponds pas au nombre de fichiers dans le repertoire Images\"\n",
    "\n",
    "# Check if the uniq_id column has the same names than the files in the Images directory\n",
    "files_without_jpg = [file.strip(\".jpg\") for file in os.listdir(\"./Images\")]\n",
    "assert (\n",
    "    flipkart[\"uniq_id\"].isin(files_without_jpg).all()\n",
    "), \"Les fichiers dans le repertoire Images ne correspondent pas aux fichiers dans le fichier flipkart.csv\"\n",
    "print(\"La colonne uniq_id contient les mêmes noms que les fichiers dans le répertoire Images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La colonne image est égale à la colonne uniq_id sans l'extension .jpg\n"
     ]
    }
   ],
   "source": [
    "# Check if the image column is equal to the uniq_id column with the .jpg extension\n",
    "assert flipkart[\"uniq_id\"].equals(\n",
    "    flipkart[\"image\"].str.strip(\".jpg\")\n",
    "), \"les colonnes image sans l'extension .jpg et uniq_id ne correspondent pas\"\n",
    "print(\"La colonne image est égale à la colonne uniq_id sans l'extension .jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les formats d'images sont:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1\n",
       "jpg    1050\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the image column contains different image formats\n",
    "print(\"Les formats d'images sont:\")\n",
    "flipkart['image'].str.rsplit('.', expand=True)[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La colonne product_rating est égale à la colonne overall_rating\n"
     ]
    }
   ],
   "source": [
    "# Check if the product_rating column is equal to the overall_rating column\n",
    "assert flipkart[\"product_rating\"].equals(\n",
    "    flipkart[\"overall_rating\"]\n",
    "), \"les colonnes product_rating et overall_rating ne correspondent pas\"\n",
    "print(\"La colonne product_rating est égale à la colonne overall_rating\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des catégories de produits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nous allons extraire les catégories de produits pour pouvoir les utiliser dans la classification des produits.**<br>\n",
    "Chaque catégorie est séparée par un '>>' et nous allons les séparer et les stocker dans une colonne 'category_1', 'category_2', 'category_3' etc...<br>\n",
    "La colonne 'category_1' contient la catégorie principale, 'category_2' la sous-catégorie et 'category_3' la sous-sous-catégorie, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the categories from the product_category_tree column into separate columns\n",
    "categories = (\n",
    "    flipkart[\"product_category_tree\"].str.strip('[\"|\"]')\n",
    "    .str.split(\">>\", expand=True)\n",
    "    .rename(columns={i: f'category_{i}' for i in range(0, 7)})\n",
    "    )\n",
    "\n",
    "# Concatenate the flipkart and categories dataframes and drop the product_category_tree column\n",
    "flipkart_with_cat = (\n",
    "    pd.concat([flipkart, categories], axis=1)\n",
    "    .drop(\"product_category_tree\", axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélection des colonnes utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour la suite de l'analyse, on va sélectionner les colonnes utiles pour la classification des produits.**<br>\n",
    "On travaillera sur les colonnes 'product_name', 'description' et 'product_specifications' pour les données textes et sur les colonnes 'image' pour les données images.<br>\n",
    "Concernant la colonne 'product_specifications', il est encore difficile de savoir si l'on va l'utiliser pour la classification des produits. Nous allons donc la conserver pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"uniq_id\",\n",
    "    \"product_name\",\n",
    "    \"category_0\",\n",
    "    \"category_1\",\n",
    "    \"category_2\",\n",
    "    \"category_3\",\n",
    "    \"category_4\",\n",
    "    \"category_5\",\n",
    "    \"category_6\",\n",
    "    \"retail_price\",\n",
    "    \"discounted_price\",\n",
    "    \"description\",\n",
    "    \"product_rating\",\n",
    "    \"product_specifications\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'appellerai le nouveau dataframe `flipkart_filtered` qui contiendra les colonnes utiles pour la classification des produits.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the selected fields\n",
    "flipkart_filtered = flipkart_with_cat[fields].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remplacement des ratings nuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the text \"No rating available\" by 0 in the product_rating column and convert it to float\n",
    "flipkart_filtered = flipkart_filtered.assign(**{'product_rating': lambda x: x['product_rating'].replace('No rating available', np.nan).astype(float)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification des erreurs d'encodage dans les colonnes textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai identifié un problème d'encodage dans les colonnes textuelles avec le caractère �.<br>\n",
    "Regardons les colonnes textuelles pour vérifier la présence de ce caractère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniq_id                    0.0\n",
       "product_name               3.0\n",
       "category_0                 0.0\n",
       "category_1                 0.0\n",
       "category_2                 0.0\n",
       "category_3                 0.0\n",
       "category_4                 1.0\n",
       "category_5                 0.0\n",
       "category_6                 0.0\n",
       "description               11.0\n",
       "product_specifications     4.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the problematic pattern\n",
    "pattern = \"�\"\n",
    "\n",
    "# Check if the flipkart_filtered DataFrame contains the character �\n",
    "flipkart_filtered.select_dtypes(include='object').apply(lambda x: x.str.count(pattern).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('uniq_id', []), ('product_name', ['Repouss______', '___']), ('category_0', []), ('category_1', []), ('category_2', []), ('category_3', []), ('category_4', ['___']), ('category_5', []), ('category_6', []), ('description', ['Repouss______', 'Repouss______', '___To', '___', '___To', '___', '___', 'd___cor', 'd___cor']), ('product_specifications', ['Repouss______', 'Repouss______'])]\n",
      "[('uniq_id', []), ('product_name', [' ___ ']), ('category_0', []), ('category_1', []), ('category_2', []), ('category_3', []), ('category_4', [' ___ ']), ('category_5', []), ('category_6', []), ('description', ['.___\\r\\n', '.___', ' ___ ']), ('product_specifications', [])]\n"
     ]
    }
   ],
   "source": [
    "# Create a new list\n",
    "words = []\n",
    "non_words = []\n",
    "\n",
    "# Create a pattern to match words containing the character � with a pattern like \"___\" which doesn't exist in the text\n",
    "word_pattern = r\"\\b\\w*___\\w*\\b\"\n",
    "nonword_pattern = r\"\\b\\W*___\\W*\\b\"\n",
    "\n",
    "# Add each word with word_pattern to the words list and each word with nonword_pattern to the non_words list\n",
    "for column in flipkart_filtered.select_dtypes(include='object').replace('�', '___'):\n",
    "    words.append((column, flipkart_filtered[column].dropna().str.replace('�', '___').str.findall(word_pattern).sum()))\n",
    "    non_words.append((column, flipkart_filtered[column].dropna().str.replace('�', '___').str.findall(nonword_pattern).sum()))\n",
    "\n",
    "# Print the words and non_words lists\n",
    "print(words)\n",
    "print(non_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après inspection des passages concernée, je peux remplacer les passages spécifiques avec \"Repouss��\" et \"d�cor\" par \"Repousse\" et \"decor\" respectivement.<br>\n",
    "Pour le reste, je vais supprimer les occurences de ce caractère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pattern to match\n",
    "pattern_1 = \"Repouss��\"\n",
    "pattern_2 = \"d�cor\"\n",
    "\n",
    "# Replace the pattern_1 by \"Repoussé\" and the pattern_2 by \"décor\" in the product_specifications column\n",
    "for column in flipkart_filtered.select_dtypes(include='object'):\n",
    "    flipkart_filtered[column] = flipkart_filtered[column].str.replace(pattern_1, \"Repoussé\").str.replace(pattern_2, \"décor\").str.replace(pattern, '')\n",
    "\n",
    "# Check if the flipkart_filtered DataFrame contains the character �\n",
    "assert flipkart_filtered.select_dtypes(include='object').apply(lambda x: x.str.count(pattern).sum()).sum() == 0.0, \"Il reste encore des caractères � dans le DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA sur les colonnes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>category_0</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>category_4</th>\n",
       "      <th>category_5</th>\n",
       "      <th>category_6</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>description</th>\n",
       "      <th>product_rating</th>\n",
       "      <th>product_specifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7</td>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>Curtains &amp; Accessories</td>\n",
       "      <td>Curtains</td>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1899.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Brand\", \"v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590</td>\n",
       "      <td>Sathiyas Cotton Bath Towel</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>Baby Bath &amp; Skin</td>\n",
       "      <td>Baby Bath Towels</td>\n",
       "      <td>Sathiyas Baby Bath Towels</td>\n",
       "      <td>Sathiyas Cotton Bath Towel (3 Bath Towel, Red...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>600.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Machine Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74</td>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>Baby Bath &amp; Skin</td>\n",
       "      <td>Baby Bath Towels</td>\n",
       "      <td>Eurospa Baby Bath Towels</td>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set (20 PIECE...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Material\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d4684dcdc759dd9cdf41504698d737d8</td>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King size...</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>Bed Linen</td>\n",
       "      <td>Bedsheets</td>\n",
       "      <td>SANTOSH ROYAL FASHION Bedsheets</td>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King siz...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2699.0</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>Key Features of SANTOSH ROYAL FASHION Cotton P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Brand\", \"v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6325b6870c54cd47be6ebfbffa620ec7</td>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double B...</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>Bed Linen</td>\n",
       "      <td>Bedsheets</td>\n",
       "      <td>Jaipur Print Bedsheets</td>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2599.0</td>\n",
       "      <td>698.0</td>\n",
       "      <td>Key Features of Jaipur Print Cotton Floral Kin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Machine Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>958f54f4c46b53c8a0a9b8167d9140bc</td>\n",
       "      <td>Oren Empower Extra Large Self Adhesive Sticker</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>Baby &amp; Kids Gifts</td>\n",
       "      <td>Stickers</td>\n",
       "      <td>Oren Empower Stickers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>Oren Empower Extra Large Self Adhesive Sticker...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Number of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>fd6cbcc22efb6b761bd564c28928483c</td>\n",
       "      <td>Wallmantra Large Vinyl Sticker Sticker</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>Baby &amp; Kids Gifts</td>\n",
       "      <td>Stickers</td>\n",
       "      <td>Wallmantra Stickers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>1896.0</td>\n",
       "      <td>Wallmantra Large Vinyl Sticker Sticker (Pack o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Number of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>5912e037d12774bb73a2048f35a00009</td>\n",
       "      <td>Uberlyfe Extra Large Pigmented Polyvinyl Films...</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>Baby &amp; Kids Gifts</td>\n",
       "      <td>Stickers</td>\n",
       "      <td>Uberlyfe Stickers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>1449.0</td>\n",
       "      <td>Buy Uberlyfe Extra Large Pigmented Polyvinyl F...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Number of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>c3edc504d1b4f0ba6224fa53a43a7ad6</td>\n",
       "      <td>Wallmantra Medium Vinyl Sticker Sticker</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>Baby &amp; Kids Gifts</td>\n",
       "      <td>Stickers</td>\n",
       "      <td>Wallmantra Stickers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3465.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>Buy Wallmantra Medium Vinyl Sticker Sticker fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Number of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>f2f027ad6a6df617c9f125173da71e44</td>\n",
       "      <td>Uberlyfe Large Vinyl Sticker</td>\n",
       "      <td>Baby Care</td>\n",
       "      <td>Baby &amp; Kids Gifts</td>\n",
       "      <td>Stickers</td>\n",
       "      <td>Uberlyfe Stickers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>Buy Uberlyfe Large Vinyl Sticker for Rs.595 on...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Sales Pack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               uniq_id  \\\n",
       "0     55b85ea15a1536d46b7190ad6fff8ce7   \n",
       "1     7b72c92c2f6c40268628ec5f14c6d590   \n",
       "2     64d5d4a258243731dc7bbb1eef49ad74   \n",
       "3     d4684dcdc759dd9cdf41504698d737d8   \n",
       "4     6325b6870c54cd47be6ebfbffa620ec7   \n",
       "...                                ...   \n",
       "1045  958f54f4c46b53c8a0a9b8167d9140bc   \n",
       "1046  fd6cbcc22efb6b761bd564c28928483c   \n",
       "1047  5912e037d12774bb73a2048f35a00009   \n",
       "1048  c3edc504d1b4f0ba6224fa53a43a7ad6   \n",
       "1049  f2f027ad6a6df617c9f125173da71e44   \n",
       "\n",
       "                                           product_name        category_0  \\\n",
       "0     Elegance Polyester Multicolor Abstract Eyelet ...  Home Furnishing    \n",
       "1                            Sathiyas Cotton Bath Towel        Baby Care    \n",
       "2                   Eurospa Cotton Terry Face Towel Set        Baby Care    \n",
       "3     SANTOSH ROYAL FASHION Cotton Printed King size...  Home Furnishing    \n",
       "4     Jaipur Print Cotton Floral King sized Double B...  Home Furnishing    \n",
       "...                                                 ...               ...   \n",
       "1045     Oren Empower Extra Large Self Adhesive Sticker        Baby Care    \n",
       "1046             Wallmantra Large Vinyl Sticker Sticker        Baby Care    \n",
       "1047  Uberlyfe Extra Large Pigmented Polyvinyl Films...        Baby Care    \n",
       "1048            Wallmantra Medium Vinyl Sticker Sticker        Baby Care    \n",
       "1049                       Uberlyfe Large Vinyl Sticker        Baby Care    \n",
       "\n",
       "                    category_1          category_2  \\\n",
       "0      Curtains & Accessories            Curtains    \n",
       "1            Baby Bath & Skin    Baby Bath Towels    \n",
       "2            Baby Bath & Skin    Baby Bath Towels    \n",
       "3                   Bed Linen           Bedsheets    \n",
       "4                   Bed Linen           Bedsheets    \n",
       "...                        ...                 ...   \n",
       "1045        Baby & Kids Gifts            Stickers    \n",
       "1046        Baby & Kids Gifts            Stickers    \n",
       "1047        Baby & Kids Gifts            Stickers    \n",
       "1048        Baby & Kids Gifts            Stickers    \n",
       "1049        Baby & Kids Gifts            Stickers    \n",
       "\n",
       "                                             category_3  \\\n",
       "0      Elegance Polyester Multicolor Abstract Eyelet...   \n",
       "1                            Sathiyas Baby Bath Towels    \n",
       "2                             Eurospa Baby Bath Towels    \n",
       "3                      SANTOSH ROYAL FASHION Bedsheets    \n",
       "4                               Jaipur Print Bedsheets    \n",
       "...                                                 ...   \n",
       "1045                              Oren Empower Stickers   \n",
       "1046                                Wallmantra Stickers   \n",
       "1047                                  Uberlyfe Stickers   \n",
       "1048                                Wallmantra Stickers   \n",
       "1049                                  Uberlyfe Stickers   \n",
       "\n",
       "                                             category_4 category_5 category_6  \\\n",
       "0                                                  None       None       None   \n",
       "1      Sathiyas Cotton Bath Towel (3 Bath Towel, Red...       None       None   \n",
       "2      Eurospa Cotton Terry Face Towel Set (20 PIECE...       None       None   \n",
       "3      SANTOSH ROYAL FASHION Cotton Printed King siz...       None       None   \n",
       "4      Jaipur Print Cotton Floral King sized Double ...       None       None   \n",
       "...                                                 ...        ...        ...   \n",
       "1045                                               None       None       None   \n",
       "1046                                               None       None       None   \n",
       "1047                                               None       None       None   \n",
       "1048                                               None       None       None   \n",
       "1049                                               None       None       None   \n",
       "\n",
       "      retail_price  discounted_price  \\\n",
       "0           1899.0             899.0   \n",
       "1            600.0             449.0   \n",
       "2              NaN               NaN   \n",
       "3           2699.0            1299.0   \n",
       "4           2599.0             698.0   \n",
       "...            ...               ...   \n",
       "1045        1399.0             999.0   \n",
       "1046        4930.0            1896.0   \n",
       "1047        4500.0            1449.0   \n",
       "1048        3465.0            1333.0   \n",
       "1049        1190.0             595.0   \n",
       "\n",
       "                                            description  product_rating  \\\n",
       "0     Key Features of Elegance Polyester Multicolor ...             NaN   \n",
       "1     Specifications of Sathiyas Cotton Bath Towel (...             NaN   \n",
       "2     Key Features of Eurospa Cotton Terry Face Towe...             NaN   \n",
       "3     Key Features of SANTOSH ROYAL FASHION Cotton P...             NaN   \n",
       "4     Key Features of Jaipur Print Cotton Floral Kin...             NaN   \n",
       "...                                                 ...             ...   \n",
       "1045  Oren Empower Extra Large Self Adhesive Sticker...             NaN   \n",
       "1046  Wallmantra Large Vinyl Sticker Sticker (Pack o...             NaN   \n",
       "1047  Buy Uberlyfe Extra Large Pigmented Polyvinyl F...             NaN   \n",
       "1048  Buy Wallmantra Medium Vinyl Sticker Sticker fo...             NaN   \n",
       "1049  Buy Uberlyfe Large Vinyl Sticker for Rs.595 on...             4.0   \n",
       "\n",
       "                                 product_specifications  \n",
       "0     {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  \n",
       "1     {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  \n",
       "2     {\"product_specification\"=>[{\"key\"=>\"Material\",...  \n",
       "3     {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  \n",
       "4     {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  \n",
       "...                                                 ...  \n",
       "1045  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
       "1046  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
       "1047  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
       "1048  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
       "1049  {\"product_specification\"=>[{\"key\"=>\"Sales Pack...  \n",
       "\n",
       "[1050 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipkart_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories in category_1:\n",
      "63\n",
      "Number of categories in category_2:\n",
      "246\n",
      "Number of categories in category_3:\n",
      "350\n",
      "Number of categories in category_4:\n",
      "297\n",
      "Number of categories in category_5:\n",
      "117\n",
      "Number of categories in category_6:\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 7):\n",
    "    print(\"Number of categories in category_{}:\".format(i))\n",
    "    print(flipkart_filtered[f'category_{i}'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenante the product_name and description columns\n",
    "flipkart_filtered[\"text\"] = flipkart_filtered[\"product_name\"] + \" \" + flipkart_filtered[\"description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV - TF-IDF sur les colonnes textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données textes pour encode avec CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Création d'une instance de TfIdfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des données textes\n",
    "X = vectorizer.fit_transform(flipkart_filtered[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du nom des features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Création d'un DataFrame avec les données encodées\n",
    "text_encoded = pd.DataFrame(X.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>001</th>\n",
       "      <th>0021</th>\n",
       "      <th>004</th>\n",
       "      <th>005</th>\n",
       "      <th>006</th>\n",
       "      <th>0083</th>\n",
       "      <th>01</th>\n",
       "      <th>011</th>\n",
       "      <th>...</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zingalalaa</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipexterior</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zippered</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zora</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 6052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  001  0021  004  005  006  0083   01  011  ...  zinc  \\\n",
       "0     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "1     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "2     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "3     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "4     0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "...   ...  ...  ...   ...  ...  ...  ...   ...  ...  ...  ...   ...   \n",
       "1045  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "1046  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "1047  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "1048  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "1049  0.0  0.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "      zingalalaa  zip  zipexterior  zipper  zippered  zone  zoom  zora  zyxel  \n",
       "0            0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "1            0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "2            0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "3            0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "4            0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "...          ...  ...          ...     ...       ...   ...   ...   ...    ...  \n",
       "1045         0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "1046         0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "1047         0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "1048         0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "1049         0.0  0.0          0.0     0.0       0.0   0.0   0.0   0.0    0.0  \n",
       "\n",
       "[1050 rows x 6052 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "of             39.887814\n",
       "only           37.715187\n",
       "for            35.120723\n",
       "at             32.578276\n",
       "and            31.970817\n",
       "                 ...    \n",
       "safely          0.028043\n",
       "trip            0.028043\n",
       "portability     0.028043\n",
       "348             0.028043\n",
       "notepad         0.028043\n",
       "Length: 6052, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English tokenizer\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print the attributes of the spacy pipeline\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: curated_transformers in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from curated_transformers) (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from torch>=1.12.0->curated_transformers) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from torch>=1.12.0->curated_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from torch>=1.12.0->curated_transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from torch>=1.12.0->curated_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from torch>=1.12.0->curated_transformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from torch>=1.12.0->curated_transformers) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from torch>=1.12.0->curated_transformers) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.0->curated_transformers) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.0->curated_transformers) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from jinja2->torch>=1.12.0->curated_transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages (from sympy->torch>=1.12.0->curated_transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install curated_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English spacy Roberta model\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Print the attributes of the spacy pipeline\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer', 'ner']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove every attribute of the spacy pipeline except the lemmatizer\n",
    "nlp.disable_pipes(\"transformer\", \"ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_preprocessor(doc):\n",
    "    return \" \".join([token.text for token in nlp(doc) if not token.is_stop and token.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_preprocessor(doc):\n",
    "    return \" \".join([token.lemma_.lower() for token in nlp(doc) if not token.is_stop and token.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(preprocessor=spacy_preprocessor, min_df=5, strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.float64'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msmooth_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "Equivalent to :class:`CountVectorizer` followed by\n",
      ":class:`TfidfTransformer`.\n",
      "\n",
      "For an example of usage, see\n",
      ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\n",
      "\n",
      "For an efficiency comparison of the different feature extractors, see\n",
      ":ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
      "\n",
      "For an example of document clustering and comparison with\n",
      ":class:`~sklearn.feature_extraction.text.HashingVectorizer`, see\n",
      ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n",
      "\n",
      "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input : {'filename', 'file', 'content'}, default='content'\n",
      "    - If `'filename'`, the sequence passed as an argument to fit is\n",
      "      expected to be a list of filenames that need reading to fetch\n",
      "      the raw content to analyze.\n",
      "\n",
      "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      "      object) that is called to fetch the bytes in memory.\n",
      "\n",
      "    - If `'content'`, the input is expected to be a sequence of items that\n",
      "      can be of type string or byte.\n",
      "\n",
      "encoding : str, default='utf-8'\n",
      "    If bytes or files are given to analyze, this encoding is used to\n",
      "    decode.\n",
      "\n",
      "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "    Instruction on what to do if a byte sequence is given to analyze that\n",
      "    contains characters not of the given `encoding`. By default, it is\n",
      "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "    values are 'ignore' and 'replace'.\n",
      "\n",
      "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      "    Remove accents and perform other character normalization\n",
      "    during the preprocessing step.\n",
      "    'ascii' is a fast method that only works on characters that have\n",
      "    a direct ASCII mapping.\n",
      "    'unicode' is a slightly slower method that works on any characters.\n",
      "    None (default) means no character normalization is performed.\n",
      "\n",
      "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "    :func:`unicodedata.normalize`.\n",
      "\n",
      "lowercase : bool, default=True\n",
      "    Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "preprocessor : callable, default=None\n",
      "    Override the preprocessing (string transformation) stage while\n",
      "    preserving the tokenizing and n-grams generation steps.\n",
      "    Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "tokenizer : callable, default=None\n",
      "    Override the string tokenization step while preserving the\n",
      "    preprocessing and n-grams generation steps.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "    Whether the feature should be made of word or character n-grams.\n",
      "    Option 'char_wb' creates character n-grams only from text inside\n",
      "    word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "    If a callable is passed it is used to extract the sequence of features\n",
      "    out of the raw, unprocessed input.\n",
      "\n",
      "    .. versionchanged:: 0.21\n",
      "        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
      "        is first read from the file and then passed to the given callable\n",
      "        analyzer.\n",
      "\n",
      "stop_words : {'english'}, list, default=None\n",
      "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "    list is returned. 'english' is currently the only supported string\n",
      "    value.\n",
      "    There are several known issues with 'english' and you should\n",
      "    consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "    If a list, that list is assumed to contain stop words, all of which\n",
      "    will be removed from the resulting tokens.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    If None, no stop words will be used. In this case, setting `max_df`\n",
      "    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
      "    and filter stop words based on intra corpus document frequency of terms.\n",
      "\n",
      "token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      "    Regular expression denoting what constitutes a \"token\", only used\n",
      "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "    or more alphanumeric characters (punctuation is completely ignored\n",
      "    and always treated as a token separator).\n",
      "\n",
      "    If there is a capturing group in token_pattern then the\n",
      "    captured group content, not the entire match, becomes the token.\n",
      "    At most one capturing group is permitted.\n",
      "\n",
      "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "    The lower and upper boundary of the range of n-values for different\n",
      "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "    only bigrams.\n",
      "    Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "max_df : float or int, default=1.0\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly higher than the given threshold (corpus-specific\n",
      "    stop words).\n",
      "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      "    documents, integer absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "min_df : float or int, default=1\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly lower than the given threshold. This value is also\n",
      "    called cut-off in the literature.\n",
      "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      "    of documents, integer absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "max_features : int, default=None\n",
      "    If not None, build a vocabulary that only consider the top\n",
      "    `max_features` ordered by term frequency across the corpus.\n",
      "    Otherwise, all features are used.\n",
      "\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "vocabulary : Mapping or iterable, default=None\n",
      "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "    indices in the feature matrix, or an iterable over terms. If not\n",
      "    given, a vocabulary is determined from the input documents.\n",
      "\n",
      "binary : bool, default=False\n",
      "    If True, all non-zero term counts are set to 1. This does not mean\n",
      "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "    is binary. (Set `binary` to True, `use_idf` to False and\n",
      "    `norm` to None to get 0/1 outputs).\n",
      "\n",
      "dtype : dtype, default=float64\n",
      "    Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "norm : {'l1', 'l2'} or None, default='l2'\n",
      "    Each output row will have unit norm, either:\n",
      "\n",
      "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "      similarity between two vectors is their dot product when l2 norm has\n",
      "      been applied.\n",
      "    - 'l1': Sum of absolute values of vector elements is 1.\n",
      "      See :func:`~sklearn.preprocessing.normalize`.\n",
      "    - None: No normalization.\n",
      "\n",
      "use_idf : bool, default=True\n",
      "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
      "\n",
      "smooth_idf : bool, default=True\n",
      "    Smooth idf weights by adding one to document frequencies, as if an\n",
      "    extra document was seen containing every term in the collection\n",
      "    exactly once. Prevents zero divisions.\n",
      "\n",
      "sublinear_tf : bool, default=False\n",
      "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "vocabulary_ : dict\n",
      "    A mapping of terms to feature indices.\n",
      "\n",
      "fixed_vocabulary_ : bool\n",
      "    True if a fixed vocabulary of term to indices mapping\n",
      "    is provided by the user.\n",
      "\n",
      "idf_ : array of shape (n_features,)\n",
      "    The inverse document frequency (IDF) vector; only defined\n",
      "    if ``use_idf`` is True.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "    matrix of counts.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      ">>> corpus = [\n",
      "...     'This is the first document.',\n",
      "...     'This document is the second document.',\n",
      "...     'And this is the third one.',\n",
      "...     'Is this the first document?',\n",
      "... ]\n",
      ">>> vectorizer = TfidfVectorizer()\n",
      ">>> X = vectorizer.fit_transform(corpus)\n",
      ">>> vectorizer.get_feature_names_out()\n",
      "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      "       'this'], ...)\n",
      ">>> print(X.shape)\n",
      "(4, 9)\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\bs6517\\documents_local\\projet_6\\oc_projet_6\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"he loves working with data. It works like a charm at his work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he he True True\n",
      "loves love False True\n",
      "working work False True\n",
      "with with True True\n",
      "data datum False True\n",
      ". . False False\n",
      "It it True True\n",
      "works work False True\n",
      "like like False True\n",
      "a a True True\n",
      "charm charm False True\n",
      "at at True True\n",
      "his his True True\n",
      "work work False True\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.is_stop, token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des données textes\n",
    "X = tfidf.fit_transform(flipkart_filtered[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du nom des features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Création d'un DataFrame avec les données encodées\n",
    "text_encoded = pd.DataFrame(X.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absorbent</th>\n",
       "      <th>abstract</th>\n",
       "      <th>ac</th>\n",
       "      <th>accessory</th>\n",
       "      <th>actual</th>\n",
       "      <th>adapter</th>\n",
       "      <th>add</th>\n",
       "      <th>addition</th>\n",
       "      <th>additional</th>\n",
       "      <th>adhesive</th>\n",
       "      <th>...</th>\n",
       "      <th>wireless</th>\n",
       "      <th>woman</th>\n",
       "      <th>wood</th>\n",
       "      <th>wooden</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>wrap</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173857</td>\n",
       "      <td>0.068128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      absorbent  abstract   ac  accessory  actual  adapter  add  addition  \\\n",
       "0           0.0  0.212054  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "1           0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "2           0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "3           0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "4           0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "...         ...       ...  ...        ...     ...      ...  ...       ...   \n",
       "1045        0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "1046        0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "1047        0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "1048        0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "1049        0.0  0.000000  0.0        0.0     0.0      0.0  0.0       0.0   \n",
       "\n",
       "      additional  adhesive  ...  wireless     woman  wood  wooden  work  \\\n",
       "0            0.0  0.000000  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "1            0.0  0.000000  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "2            0.0  0.000000  ...       0.0  0.040182   0.0     0.0   0.0   \n",
       "3            0.0  0.000000  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "4            0.0  0.000000  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "...          ...       ...  ...       ...       ...   ...     ...   ...   \n",
       "1045         0.0  0.090722  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "1046         0.0  0.119800  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "1047         0.0  0.000000  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "1048         0.0  0.000000  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "1049         0.0  0.000000  ...       0.0  0.000000   0.0     0.0   0.0   \n",
       "\n",
       "         world  wrap      year    yellow       yes  \n",
       "0     0.055310   0.0  0.000000  0.000000  0.000000  \n",
       "1     0.000000   0.0  0.000000  0.173857  0.068128  \n",
       "2     0.000000   0.0  0.047634  0.000000  0.000000  \n",
       "3     0.000000   0.0  0.000000  0.000000  0.042168  \n",
       "4     0.000000   0.0  0.000000  0.000000  0.034755  \n",
       "...        ...   ...       ...       ...       ...  \n",
       "1045  0.000000   0.0  0.000000  0.000000  0.000000  \n",
       "1046  0.109059   0.0  0.000000  0.000000  0.000000  \n",
       "1047  0.000000   0.0  0.000000  0.000000  0.000000  \n",
       "1048  0.000000   0.0  0.000000  0.000000  0.000000  \n",
       "1049  0.000000   0.0  0.000000  0.000000  0.000000  \n",
       "\n",
       "[1050 rows x 901 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "watch          41.628355\n",
       "analog         37.200037\n",
       "products       31.595215\n",
       "genuine        31.328765\n",
       "shipping       31.328765\n",
       "                 ...    \n",
       "notepad         0.032455\n",
       "expandable      0.032455\n",
       "safely          0.032455\n",
       "confidently     0.032455\n",
       "distribute      0.032455\n",
       "Length: 4727, dtype: float64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1845161307808877"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded['aa'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
