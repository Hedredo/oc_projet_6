{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Livrable** : Travail de veille et preuve de concept d’une technique récente et conclusion (4 minutes) <br>\n",
    "Attention, le contenu de ce livrable n'est pas clairement défini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CGVIT TINY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    LayerNormalization,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    MultiHeadAttention,\n",
    "    Layer,\n",
    "    Add,\n",
    "    Flatten,\n",
    "    RandomFlip,\n",
    "    RandomRotation,\n",
    "    RandomContrast,\n",
    "    RandomBrightness\n",
    ")\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autres imports\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output, display\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Définit le nombre de coeurs utilisés à 10\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dataframe flipkart a été chargé lors du préprocessing dans les globals, réassignons le au nom flipkart pour plus de clarté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050 entries, 0 to 1049\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   image   1050 non-null   object  \n",
      " 1   class   1050 non-null   category\n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 9.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "image_df = globals()['flipkart']\n",
    "image_df.drop(columns=['product_name', 'description'], inplace=True)\n",
    "print(image_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigne le nombre de classes\n",
    "n_classes = image_df['class'].nunique()\n",
    "\n",
    "# Assigne la liste des classes\n",
    "labels = list(image_df['class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Séparation des données en train-test-split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION DES HYPERPARAMETRES DU MODELE\n",
    "FILENAME_VAR = 'image'\n",
    "CLASS_VAR = 'class'\n",
    "# TAILLE DES JEUX DE DONNEES\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.2\n",
    "# SEED\n",
    "SEED = 314\n",
    "BATCH_SIZE = 32\n",
    "SIZE = 224\n",
    "CHANNELS = 3\n",
    "LABELS = list(image_df['class'].unique())\n",
    "NLABELS = len(LABELS)\n",
    "EPOCHS = 3\n",
    "LR = 1e-3\n",
    "IN_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "NPATCHES = 256\n",
    "NHEADS = 4\n",
    "NLAYERS = 2\n",
    "HIDDEN_SIZE = 768\n",
    "DENSE_UNITS = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 672\n",
      "Validation size: 168\n",
      "Test size: 210\n"
     ]
    }
   ],
   "source": [
    "# Création du jeu de données de test et temporaire (train + validation)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    image_df[FILENAME_VAR], image_df[CLASS_VAR],\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED, stratify=image_df[CLASS_VAR])\n",
    "\n",
    "# Création du jeu de données de train et de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=VAL_SIZE,\n",
    "    random_state=SEED, stratify=y_temp)\n",
    "\n",
    "# Supprime les jeux de données temporaires\n",
    "del X_temp, y_temp\n",
    "gc.collect()\n",
    "\n",
    "# Regroupement des données en dataframe de train et de test\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "val = pd.concat([X_val, y_val], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Supprime les jeux de données temporaires\n",
    "del X_train, y_train, X_val, y_val, X_test, y_test\n",
    "gc.collect()\n",
    "\n",
    "# Affiche les tailles des jeux de données\n",
    "print(f\"Train size: {train.shape[0]}\")\n",
    "print(f\"Validation size: {val.shape[0]}\")\n",
    "print(f\"Test size: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GCViT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m ckpt_link \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/awsaf49/gcvit-tf/releases/download/v1.1.6/gcvitxxtiny.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Build Model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGCViT\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m     16\u001b[0m inp \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m)))\n\u001b[0;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m model(inp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GCViT' is not defined"
     ]
    }
   ],
   "source": [
    "# Model Configs\n",
    "config = {\n",
    "    \"window_size\": (7, 7, 14, 7),\n",
    "    \"embed_dim\": 64,\n",
    "    \"depths\": (2, 2, 6, 2),\n",
    "    \"num_heads\": (2, 4, 8, 16),\n",
    "    \"mlp_ratio\": 3.0,\n",
    "    \"path_drop\": 0.2,\n",
    "}\n",
    "ckpt_link = (\n",
    "    \"https://github.com/awsaf49/gcvit-tf/releases/download/v1.1.6/gcvitxxtiny.keras\"\n",
    ")\n",
    "\n",
    "# Build Model\n",
    "model = GCViT(**config)\n",
    "inp = ops.array(np.random.uniform(size=(1, 224, 224, 3)))\n",
    "out = model(inp)\n",
    "\n",
    "# Load Weights\n",
    "ckpt_path = keras.utils.get_file(ckpt_link.split(\"/\")[-1], ckpt_link)\n",
    "model.load_weights(ckpt_path)\n",
    "\n",
    "# Summary\n",
    "model.summary((224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger une image à partir du chemin et du label\n",
    "def preprocess_image_and_load_label(filename, label, SIZE, CHANNELS, preprocessing_func):\n",
    "    # Vérifie que preprocessing prends une fonction si n'est pas None\n",
    "    if preprocessing_func is not None and not callable(preprocessing_func):\n",
    "        raise ValueError(\"preprocessing doit être une fonction\")\n",
    "    \n",
    "    # Effectue le prétraitement de l'image\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
    "    image = tf.image.resize(image, [SIZE, SIZE])\n",
    "    if preprocessing_func is not None:\n",
    "        try:\n",
    "            image = preprocessing_func(image)\n",
    "        except Exception as e:\n",
    "            print(\"Vérifiez la fonction de preprocessing\")\n",
    "            raise e\n",
    "    return image, label\n",
    "\n",
    "# Fonction pour transformer un dataframe en tf.data.Dataset\n",
    "def df_to_tfdataset(df, SIZE, BATCH_SIZE, CHANNELS, preprocessing_func=None):\n",
    "    filenames = df[FILENAME_VAR].values\n",
    "    labels = df[CLASS_VAR].values\n",
    "    tfdataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "    # Appliquer la fonction de prétraitement\n",
    "    tfdataset = tfdataset.map(\n",
    "        lambda filename, label: preprocess_image_and_load_label(filename, label, SIZE, CHANNELS, preprocessing_func),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "    # Mélanger et mettre en batch\n",
    "    tfdataset = tfdataset.shuffle(buffer_size=len(df)).batch(BATCH_SIZE)\n",
    "    return tfdataset\n",
    "\n",
    "# Shuffle et batcher le dataset\n",
    "train_dataset = df_to_tfdataset(train, SIZE, BATCH_SIZE, CHANNELS, efficientnet_preprocess_input)\n",
    "val_dataset = df_to_tfdataset(val, SIZE, BATCH_SIZE, CHANNELS, efficientnet_preprocess_input)\n",
    "test_dataset = df_to_tfdataset(test, SIZE, BATCH_SIZE, CHANNELS, efficientnet_preprocess_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel, AutoImageProcessor, TFViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTModel: ['encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.weight']\n",
      "- This IS expected if you are initializing TFViTModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFViTModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Création d'une configuration personnalisée pour le modèle\n",
    "custom_config = ViTConfig(\n",
    "    image_size = SIZE,\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    num_attention_heads = NHEADS,\n",
    "    num_hidden_layers = NLAYERS\n",
    ")\n",
    "\n",
    "# Chargement du modèle pré-entraîné avec la configuration\n",
    "custom_model = TFViTModel.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k',\n",
    "    config=custom_config,\n",
    "    name='vit_custom'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTModel.\n",
      "\n",
      "All the weights of TFViTModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle pré-entraîné avec la configuration\n",
    "base_model = TFViTModel.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k',\n",
    "    name='vit'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
       "  \"architectures\": [\n",
       "    \"ViTModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.44.0\"\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import(\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    LayerNormalization,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Embedding,\n",
    "    MultiHeadAttention,\n",
    "    Layer,\n",
    "    Add,\n",
    "    Resizing,\n",
    "    Rescaling,\n",
    "    Permute,\n",
    "    Flatten,\n",
    "    RandomFlip,\n",
    "    RandomRotation,\n",
    "    RandomContrast,\n",
    "    RandomBrightness\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = tf.keras.Sequential([\n",
    "    Resizing(SIZE, SIZE),\n",
    "    Rescaling(1./255),\n",
    "    Permute((3, 1, 2)) # Permute les dimensions en (3, 224, 224)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomRotation(0.2),\n",
    "    RandomContrast(0.2),\n",
    "    RandomBrightness(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'vit' (type TFViTModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for pixel_values.\n\nCall arguments received by layer 'vit' (type TFViTModel):\n  • pixel_values=<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=None, name=keras_tensor_34>\n  • head_mask=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • interpolate_pos_encoding=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(SIZE, SIZE, CHANNELS))\n\u001b[0;32m      2\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m Dense(NLABELS, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:566\u001b[0m, in \u001b[0;36minput_processing\u001b[1;34m(func, config, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m         output[main_input_name] \u001b[38;5;241m=\u001b[39m main_input\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(main_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m         )\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# Populates any unspecified argument with their default value, according to the signature.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m parameter_names:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'vit' (type TFViTModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for pixel_values.\n\nCall arguments received by layer 'vit' (type TFViTModel):\n  • pixel_values=<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=None, name=keras_tensor_34>\n  • head_mask=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • interpolate_pos_encoding=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(SIZE, SIZE, CHANNELS))\n",
    "base_model.trainable = False\n",
    "x = base_model(inputs, training=False)[0]\n",
    "outputs = Dense(NLABELS, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: vit-b8 : /kaggle/input/vision-transformer/tensorflow2/vit-b8-classification/1\n"
     ]
    }
   ],
   "source": [
    "# Select an Image Classification model\n",
    "\n",
    "model_name = \"vit-b8\"\n",
    "\n",
    "model_handle_map = {\n",
    "  \"vit-b8\": \"/kaggle/input/vision-transformer/tensorflow2/vit-b8-classification/1\",\n",
    "#   \"evit-b8\": \"/kaggle/input/vision-transformer/tensorflow2/vit-b8-classification/1\",\n",
    "#   \"vit-b8\": \"/kaggle/input/efficientnet-v2/tensorflow2/imagenet1k-b0-classification/versions/1\",\n",
    "#   \"vit-b8\": \"https://kaggle.com/models/kaggle/vision-transformer/frameworks/TensorFlow2/variations/vit-b8-classification/\",\n",
    "#   \"vit-b8\": \"https://kaggle.com/models/kaggle/vision-transformer/frameworks/TensorFlow2/variations/vit-b8-classification/versions/1\",\n",
    "#   \"vit-b8\": \"https://tfhub.dev/sayakpaul/vit_b8_classification/1\",\n",
    "}\n",
    "\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"vit-b8\": 224,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map[model_name]\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: vit-b8 : /kaggle/input/vision-transformer/tensorflow2/vit-b8-classification/1\n",
      "Input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "model_image_size_map = {\n",
    "  \"vit-b8\": 224,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 16#@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home Furnishing', 'Baby Care', 'Baby Care', 'Home Furnishing', 'Home Furnishing', ..., 'Baby Care', 'Baby Care', 'Baby Care', 'Baby Care', 'Baby Care']\n",
       "Length: 1050\n",
       "Categories (7, object): ['Baby Care', 'Beauty and Personal Care', 'Computers', 'Home Decor & Festive Needs', 'Home Furnishing', 'Kitchen & Dining', 'Watches']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with /kaggle/input/vision-transformer/tensorflow2/vit-b8-classification/1\n",
      "WARNING:tensorflow:From c:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:498: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/kaggle/input/vision-transformer/tensorflow2/vit-b8-classification/1 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m do_fine_tuning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding model with\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_handle)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Explicitly define the input shape so the model can be properly\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# loaded by the TFLiteConverter\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInputLayer(input_shape\u001b[38;5;241m=\u001b[39mIMAGE_SIZE \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m3\u001b[39m,)),\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKerasLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_fine_tuning\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      9\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     10\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mlen\u001b[39m(labels),\n\u001b[0;32m     11\u001b[0m                           kernel_regularizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m0.0001\u001b[39m))\n\u001b[0;32m     12\u001b[0m ])\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild((\u001b[38;5;28;01mNone\u001b[39;00m,)\u001b[38;5;241m+\u001b[39mIMAGE_SIZE\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m3\u001b[39m,))\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:165\u001b[0m, in \u001b[0;36mKerasLayer.__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m data_structures\u001b[38;5;241m.\u001b[39mNoDependency(\n\u001b[0;32m    162\u001b[0m       _convert_nest_to_shapes(output_shape))\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func \u001b[38;5;241m=\u001b[39m \u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hub_module_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Update with the defaults when using legacy TF1 Hub format.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:467\u001b[0m, in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# Expected before TF2.4.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m       set_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[1;32m--> 467\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_load_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a string, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m handle)\n\u001b[1;32m--> 100\u001b[0m module_path \u001b[38;5;241m=\u001b[39m \u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m is_hub_module_v1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(_get_module_proto_path(module_path))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_hub_module_v1:\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001b[0m, in \u001b[0;36mresolve\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(handle):\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a module handle into a path.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    A string representing the Module path.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tensorflow_hub\\registry.py:49\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\BS6517\\Documents_local\\projet_6\\oc_projet_6\\.venv\\Lib\\site-packages\\tensorflow_hub\\resolver.py:499\u001b[0m, in \u001b[0;36mPathResolver.__call__\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, handle):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mExists(handle):\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m handle)\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
      "\u001b[1;31mOSError\u001b[0m: /kaggle/input/vision-transformer/tensorflow2/vit-b8-classification/1 does not exist."
     ]
    }
   ],
   "source": [
    "do_fine_tuning = False \n",
    "\n",
    "print(\"Building model with\", model_handle)\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(len(labels),\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
